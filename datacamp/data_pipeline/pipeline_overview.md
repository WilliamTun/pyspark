# Spark data pipeline

consist of inputs, transformations and outputs

inputs:
- csv/json/databases read into spark df

transforms:
- eg. withColumn, filter, drop

outputs:
- csv, parquet, database

validation:
- run test of data that it is as expected

analysis:
- row counts, specific calculations 

